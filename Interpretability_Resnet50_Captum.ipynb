{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sound-assignment",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torchvision\n",
    "from torchvision import models\n",
    "from torchvision import transforms\n",
    "\n",
    "from captum.attr import IntegratedGradients\n",
    "from captum.attr import GradientShap\n",
    "from captum.attr import Occlusion\n",
    "from captum.attr import Saliency\n",
    "from captum.attr import GuidedGradCam\n",
    "from captum.attr import visualization as viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "revolutionary-tulsa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "model = torch.load('Dashboard/model_conv_resnet50.pth', map_location=device)\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inclusive-consortium",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "img = Image.open(\"data/validation/malignant/ISIC_1959939.jpg\")\n",
    "\n",
    "#preprocess = transforms.Compose([\n",
    "#        transforms.Resize(256),\n",
    "#        transforms.CenterCrop(224),\n",
    "#        transforms.ToTensor(),\n",
    "#        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "#])\n",
    "\n",
    "#img = preprocess(im)\n",
    "#img = img.unsqueeze(0)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "transformed_img = transform(img)\n",
    "\n",
    "input = transformed_img.unsqueeze(0)\n",
    "\n",
    "imgplot = plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceramic-circle",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array(open(\"class.txt\").read().splitlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "curious-lover",
   "metadata": {},
   "outputs": [],
   "source": [
    "input.requires_grad = True\n",
    "output = model(input)\n",
    "output = F.softmax(output, dim=1)\n",
    "prediction_score, pred_label_idx = torch.topk(output, 1)\n",
    "\n",
    "pred_label_idx.squeeze_()\n",
    "predicted_label = labels[pred_label_idx.squeeze_()]\n",
    "print('Predicted:', predicted_label, '(', prediction_score.squeeze().item(), ')')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "favorite-wells",
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_normalize = transforms.Normalize(\n",
    "   mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],\n",
    "   std=[1/0.229, 1/0.224, 1/0.225]\n",
    ")\n",
    "\n",
    "original_image = inv_normalize(input)\n",
    "original_image1 = np.transpose(original_image.squeeze().detach().numpy(), (1,2,0))\n",
    "\n",
    "#_ = viz.visualize_image_attr(None, original_image1, method=\"original_image\", title=\"Original Image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adequate-membership",
   "metadata": {},
   "source": [
    "# Saliency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorporated-tactics",
   "metadata": {},
   "outputs": [],
   "source": [
    "saliency = Saliency(model)\n",
    "grads = saliency.attribute(input, target=pred_label_idx)\n",
    "grads = np.transpose(grads.squeeze().cpu().detach().numpy(), (1, 2, 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continuing-alpha",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = viz.visualize_image_attr_multiple(grads, original_image1, methods=[\"original_image\", \"blended_heat_map\"],signs=[\"all\", \"absolute_value\"],\n",
    "                          show_colorbar=True, titles = [\"Original Image\", \"Saliency\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "varying-yacht",
   "metadata": {},
   "source": [
    "# Integrated Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eastern-providence",
   "metadata": {},
   "outputs": [],
   "source": [
    "ig = IntegratedGradients(model)\n",
    "attr_ig = ig.attribute(input, target=pred_label_idx, n_steps=200)\n",
    "attr_ig = np.transpose(attr_ig.squeeze().cpu().detach().numpy(), (1, 2, 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "architectural-frank",
   "metadata": {},
   "outputs": [],
   "source": [
    "#_ = viz.visualize_image_attr(attr_ig, original_image1, method=\"blended_heat_map\",sign=\"all\", show_colorbar=True, title=\"Overlayed Integrated Gradients\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collective-doctor",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = viz.visualize_image_attr_multiple(attr_ig, original_image1, methods=[\"original_image\", \"blended_heat_map\"],signs=[\"all\", \"absolute_value\"],\n",
    "                          show_colorbar=True, titles = [\"Original Image\", \"Integrated Gradients\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hungarian-island",
   "metadata": {},
   "source": [
    "# Gradient Shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surface-devon",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "gradient_shap = GradientShap(model)\n",
    "\n",
    "# Defining baseline distribution of images\n",
    "rand_img_dist = torch.cat([input * 0, input * 1])\n",
    "\n",
    "attributions_gs = gradient_shap.attribute(input,\n",
    "                                          n_samples=50,\n",
    "                                          stdevs=0.0001,\n",
    "                                          baselines=rand_img_dist,\n",
    "                                          target=pred_label_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suburban-lebanon",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = viz.visualize_image_attr_multiple(np.transpose(attributions_gs.squeeze().cpu().detach().numpy(), (1,2,0)),\n",
    "                                      original_image1,\n",
    "                                      [\"original_image\", \"blended_heat_map\"],\n",
    "                                      [\"all\", \"absolute_value\"],\n",
    "                                      show_colorbar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forward-landscape",
   "metadata": {},
   "source": [
    "# Guided GradCAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "local-kazakhstan",
   "metadata": {},
   "outputs": [],
   "source": [
    "guided_gc = GuidedGradCam(model, model.layer4)\n",
    "attr_guided_gc = guided_gc.attribute(input, target=pred_label_idx)\n",
    "attr_guided_gc = np.transpose(attr_guided_gc.squeeze().cpu().detach().numpy(), (1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respected-velvet",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = viz.visualize_image_attr_multiple(attr_guided_gc, original_image1, methods=[\"original_image\", \"blended_heat_map\"],signs=[\"all\", \"absolute_value\"],\n",
    "                          show_colorbar=True, titles = [\"Original Image\", \"Occlusion\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "behind-globe",
   "metadata": {},
   "source": [
    "# Occlusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fitting-belief",
   "metadata": {},
   "outputs": [],
   "source": [
    "occlusion = Occlusion(model)\n",
    "attributions_occ = occlusion.attribute(input, strides = (3, 8, 8), target=pred_label_idx, sliding_window_shapes=(3,15, 15), baselines=0)\n",
    "attributions_occ = np.transpose(attributions_occ.squeeze().cpu().detach().numpy(), (1, 2, 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "square-deficit",
   "metadata": {},
   "outputs": [],
   "source": [
    "#_ = viz.visualize_image_attr(attributions_occ, original_image1, method=\"blended_heat_map\",sign=\"absolute_value\", show_colorbar=True, title=\"Occlusion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indoor-mountain",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = viz.visualize_image_attr_multiple(attributions_occ, original_image1, methods=[\"original_image\", \"blended_heat_map\"],signs=[\"all\", \"absolute_value\"],\n",
    "                          show_colorbar=True, titles = [\"Original Image\", \"Occlusion\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brilliant-professor",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = viz.visualize_image_attr_multiple(attributions_occ, original_image1, methods=[\"original_image\", \"blended_heat_map\", \"blended_heat_map\"],signs=[\"all\", \"absolute_value\", \"all\"],\n",
    "                          show_colorbar=True, titles = [\"Original Image\", \"Occlusion Absolute\", \"Occlusion All\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sweet-basics",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
